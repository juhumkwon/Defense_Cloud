{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMW1ol0PQB0d+WEZPL3Vc63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/Defense_Cloud/blob/main/9_0_2_Word_embedding_Positional_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1tI-kM6DxVp",
        "outputId": "c7d07ec9-0f6f-480b-86c5-1c4d6dc938b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_ids: tf.Tensor([42 23 77], shape=(3,), dtype=int32)\n",
            "\n",
            "embedded_tokens: tf.Tensor(\n",
            "[[ 0.01057028  0.01876489  0.04978373 ...  0.01247923 -0.03887016\n",
            "   0.03383276]\n",
            " [-0.01171038 -0.04157472 -0.03382719 ...  0.00186025 -0.00987711\n",
            "   0.0023483 ]\n",
            " [ 0.0409399   0.04031232 -0.00321622 ...  0.00821297 -0.01329632\n",
            "   0.04031802]], shape=(3, 512), dtype=float32)\n",
            "\n",
            "임베딩 벡터 (단어 의미 + 포지셔널 인코딩):\n",
            "input_with_positional_encoding: [[ 0.01057028  0.01876489  0.04978373 ...  0.01247923 -0.03887016\n",
            "   0.03383276]\n",
            " [ 0.82976055  0.79989624  0.7880291  ...  0.00196771 -0.00977344\n",
            "   0.00245197]\n",
            " [ 0.95023733  0.9496097   0.9331985  ...  0.00842789 -0.01308899\n",
            "   0.04052535]]\n",
            "\n",
            "Query shape: (3, 512)\n",
            "Key shape: (3, 512)\n",
            "Value shape: (3, 512)\n",
            "attention_scores: (3, 3)\n",
            "\n",
            "attention_weights: tf.Tensor(\n",
            "[[0.31941542 0.33810255 0.3424821 ]\n",
            " [0.46646982 0.35079578 0.18273439]\n",
            " [0.44185343 0.38997218 0.16817433]], shape=(3, 3), dtype=float32)\n",
            "\n",
            "\n",
            "어텐션 후 출력 벡터:\n",
            "[[-0.01899781  0.07093623 -0.4377731  ...  0.05189446  0.24394311\n",
            "  -0.06936006]\n",
            " [-0.01829063  0.06046973 -0.31675008 ...  0.0464981   0.17220637\n",
            "  -0.05856485]\n",
            " [-0.02106236  0.05983847 -0.32519194 ...  0.05034216  0.17518705\n",
            "  -0.06018689]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 1. 문장 처리 (토큰화, 임베딩, 포지셔널 인코딩 추가)\n",
        "\n",
        "# 예시 문장\n",
        "tokens = [\"나는\", \"학교에\", \"간다\"]\n",
        "\n",
        "# 어휘 크기와 임베딩 차원 정의\n",
        "vocab_size = 10000  # 어휘 크기\n",
        "embedding_dim = 512  # 임베딩 차원\n",
        "\n",
        "# 예시 토큰 ID (가정)\n",
        "token_ids = [42, 23, 77]\n",
        "token_ids = tf.constant(token_ids)\n",
        "print(\"token_ids:\", token_ids)\n",
        "print()\n",
        "\n",
        "# 임베딩 레이어 생성\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "\n",
        "# 토큰 ID → 임베딩 벡터\n",
        "# 여기서 embedded_tokens는 문장의 각 단어를 임베딩 벡터로 표현한 결과입니다.\n",
        "embedded_tokens = embedding_layer(token_ids)  # shape: (3, 512)\n",
        "print(\"embedded_tokens:\", embedded_tokens)\n",
        "print()\n",
        "\n",
        "# 포지셔널 인코딩 생성 함수\n",
        "def get_positional_encoding(seq_len, embedding_dim):\n",
        "    positions = tf.range(seq_len, dtype=tf.float32)  # 문장 길이에 대한 위치\n",
        "    i = tf.range(embedding_dim, dtype=tf.float32)  # 임베딩 차원\n",
        "    angle_rates = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(embedding_dim, tf.float32))\n",
        "    angles = positions[:, tf.newaxis] * angle_rates[tf.newaxis, :]\n",
        "    return tf.concat([tf.sin(angles), tf.cos(angles)], axis=-1)[:, :embedding_dim]\n",
        "\n",
        "# 포지셔널 인코딩 추가\n",
        "seq_len = len(tokens)\n",
        "positional_encoding = get_positional_encoding(seq_len, embedding_dim)\n",
        "input_with_positional_encoding = embedded_tokens + positional_encoding\n",
        "\n",
        "print(\"임베딩 벡터 (단어 의미 + 포지셔널 인코딩):\")\n",
        "print(\"input_with_positional_encoding:\", input_with_positional_encoding.numpy())\n",
        "print()\n",
        "\n",
        "# 2. 자기-어텐션 (Self-Attention) 계산\n",
        "# Query, Key, Value 생성\n",
        "query = tf.keras.layers.Dense(embedding_dim)(input_with_positional_encoding)  # shape: (3, 512)\n",
        "key = tf.keras.layers.Dense(embedding_dim)(input_with_positional_encoding)  # shape: (3, 512)\n",
        "value = tf.keras.layers.Dense(embedding_dim)(input_with_positional_encoding)  # shape: (3, 512)\n",
        "\n",
        "print(\"Query shape:\", query.shape)\n",
        "print(\"Key shape:\", key.shape)\n",
        "print(\"Value shape:\", value.shape)\n",
        "\n",
        "# 어텐션 스코어 계산 (Q * K^T)\n",
        "attention_scores = tf.matmul(query, key, transpose_b=True)  # shape: (3, 3)\n",
        "print(\"attention_scores:\", attention_scores.shape)\n",
        "print()\n",
        "\n",
        "# 소프트맥스를 통해 가중치 계산\n",
        "attention_weights = tf.nn.softmax(attention_scores, axis=-1)  # shape: (3, 3)\n",
        "print(\"attention_weights:\", attention_weights)\n",
        "print()\n",
        "\n",
        "# 어텐션 출력 (Attention Weight * Value)\n",
        "attention_output = tf.matmul(attention_weights, value)  # shape: (3, 512)\n",
        "\n",
        "# 3. 결과 출력\n",
        "\n",
        "print(\"\\n어텐션 후 출력 벡터:\")\n",
        "print(attention_output.numpy())\n"
      ]
    }
  ]
}