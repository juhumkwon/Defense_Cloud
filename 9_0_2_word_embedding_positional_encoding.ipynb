{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmTxed9zBW2m4lGSnfpX8W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/Defense_Cloud/blob/main/9_0_2_word_embedding_positional_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1tI-kM6DxVp",
        "outputId": "0a874152-7efa-4f96-9fe7-d42891398842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_ids: tf.Tensor([42 23 77], shape=(3,), dtype=int32)\n",
            "\n",
            "embedded_tokens: tf.Tensor(\n",
            "[[ 0.04419316 -0.01610865  0.02883747 ...  0.01627636  0.00515466\n",
            "  -0.00834884]\n",
            " [-0.02877828 -0.01684544 -0.03997324 ... -0.00483409 -0.01849253\n",
            "   0.02204117]\n",
            " [-0.04094719 -0.03268226 -0.00014751 ...  0.04200516  0.04473258\n",
            "  -0.04679233]], shape=(3, 512), dtype=float32)\n",
            "\n",
            "pos_encoding_shape: tf.Tensor(\n",
            "[[ 0.0000000e+00  1.0000000e+00  0.0000000e+00 ...  1.0000000e+00\n",
            "   0.0000000e+00  1.0000000e+00]\n",
            " [ 8.4147096e-01  5.4030228e-01  8.2185626e-01 ...  1.0000000e+00\n",
            "   1.0366329e-04  1.0000000e+00]\n",
            " [ 9.0929741e-01 -4.1614681e-01  9.3641472e-01 ...  1.0000000e+00\n",
            "   2.0732658e-04  1.0000000e+00]], shape=(3, 512), dtype=float32)\n",
            "\n",
            "임베딩 벡터 (단어 의미 + 포지셔널 인코딩):\n",
            "input_with_positional_encoding: [[ 0.04419316  0.98389137  0.02883747 ...  1.0162764   0.00515466\n",
            "   0.9916512 ]\n",
            " [ 0.8126927   0.5234568   0.781883   ...  0.9951659  -0.01838887\n",
            "   1.0220412 ]\n",
            " [ 0.8683502  -0.44882908  0.9362672  ...  1.0420052   0.04493991\n",
            "   0.9532077 ]]\n",
            "\n",
            "Query shape: (3, 512)\n",
            "Key shape: (3, 512)\n",
            "Value shape: (3, 512)\n",
            "attention_scores: (3, 3)\n",
            "\n",
            "attention_weights: tf.Tensor(\n",
            "[[0.71338785 0.1732749  0.11333731]\n",
            " [0.8612629  0.11263875 0.02609833]\n",
            " [0.95672315 0.04092748 0.0023493 ]], shape=(3, 3), dtype=float32)\n",
            "\n",
            "\n",
            "어텐션 후 출력 벡터:\n",
            "[[-0.35101002 -0.31625673  0.4596337  ... -1.0019488   0.1285649\n",
            "  -0.00594275]\n",
            " [-0.38894647 -0.37150675  0.4393089  ... -0.9512156   0.10728573\n",
            "  -0.04787488]\n",
            " [-0.40488175 -0.39585218  0.42723826 ... -0.9231156   0.09766413\n",
            "  -0.07021706]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 1. 문장 처리 (토큰화, 임베딩, 포지셔널 인코딩 추가)\n",
        "\n",
        "# 예시 문장\n",
        "tokens = [\"나는\", \"학교에\", \"간다\"]\n",
        "\n",
        "# 어휘 크기와 임베딩 차원 정의\n",
        "vocab_size = 10000  # 어휘 크기\n",
        "embedding_dim = 512  # 임베딩 차원\n",
        "\n",
        "# 예시 토큰 ID (가정)\n",
        "token_ids = [42, 23, 77]\n",
        "token_ids = tf.constant(token_ids)\n",
        "print(\"token_ids:\", token_ids)\n",
        "print()\n",
        "\n",
        "# 임베딩 레이어 생성\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "\n",
        "# 토큰 ID → 임베딩 벡터\n",
        "# 여기서 embedded_tokens는 문장의 각 단어를 임베딩 벡터로 표현한 결과입니다.\n",
        "embedded_tokens = embedding_layer(token_ids)  # shape: (3, 512)\n",
        "print(\"embedded_tokens:\", embedded_tokens)\n",
        "print()\n",
        "\n",
        "# 포지셔널 인코딩 생성 함수\n",
        "def get_positional_encoding(seq_len, embedding_dim):\n",
        "    # 각 위치 (seq_len,) -> (seq_len, 1)\n",
        "    positions = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
        "    # 임베딩 차원 인덱스 (embedding_dim,) -> (1, embedding_dim)\n",
        "    i = tf.range(embedding_dim, dtype=tf.float32)[tf.newaxis, :]\n",
        "\n",
        "    # angle rate 계산 (1 / 10000^(2i/d_model))\n",
        "    angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(embedding_dim, tf.float32))\n",
        "    angles = positions * angle_rates  # shape: (seq_len, embedding_dim)\n",
        "\n",
        "    # 짝수 인덱스는 sin, 홀수 인덱스는 cos\n",
        "    pos_encoding = tf.where(\n",
        "        tf.cast(i, tf.int32) % 2 == 0,\n",
        "        tf.sin(angles),\n",
        "        tf.cos(angles)\n",
        "    )\n",
        "\n",
        "    print(\"pos_encoding_shape:\", pos_encoding)\n",
        "    print()\n",
        "\n",
        "    return pos_encoding  # shape: (seq_len, embedding_dim)\n",
        "\n",
        "\n",
        "# 포지셔널 인코딩 추가\n",
        "seq_len = len(tokens)\n",
        "positional_encoding = get_positional_encoding(seq_len, embedding_dim)\n",
        "input_with_positional_encoding = embedded_tokens + positional_encoding\n",
        "\n",
        "print(\"임베딩 벡터 (단어 의미 + 포지셔널 인코딩):\")\n",
        "print(\"input_with_positional_encoding:\", input_with_positional_encoding.numpy())\n",
        "print()\n",
        "\n",
        "# 2. 자기-어텐션 (Self-Attention) 계산\n",
        "# Query, Key, Value 생성\n",
        "query = tf.keras.layers.Dense(embedding_dim)(input_with_positional_encoding)  # shape: (3, 512)\n",
        "key = tf.keras.layers.Dense(embedding_dim)(input_with_positional_encoding)  # shape: (3, 512)\n",
        "value = tf.keras.layers.Dense(embedding_dim)(input_with_positional_encoding)  # shape: (3, 512)\n",
        "\n",
        "print(\"Query shape:\", query.shape)\n",
        "print(\"Key shape:\", key.shape)\n",
        "print(\"Value shape:\", value.shape)\n",
        "\n",
        "# 어텐션 스코어 계산 (Q * K^T)\n",
        "attention_scores = tf.matmul(query, key, transpose_b=True)  # shape: (3, 3)\n",
        "print(\"attention_scores:\", attention_scores.shape)\n",
        "print()\n",
        "\n",
        "# 소프트맥스를 통해 가중치 계산\n",
        "attention_weights = tf.nn.softmax(attention_scores, axis=-1)  # shape: (3, 3)\n",
        "print(\"attention_weights:\", attention_weights)\n",
        "print()\n",
        "\n",
        "# 어텐션 출력 (Attention Weight * Value)\n",
        "attention_output = tf.matmul(attention_weights, value)  # shape: (3, 512)\n",
        "\n",
        "# 3. 결과 출력\n",
        "\n",
        "print(\"\\n어텐션 후 출력 벡터:\")\n",
        "print(attention_output.numpy())\n"
      ]
    }
  ]
}