{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdm06g7af96i8LZj2WoNuJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juhumkwon/Defense_Cloud/blob/main/9_1_Transformaer_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UzqOCzpBCBhO"
      },
      "outputs": [],
      "source": [
        "# 1. 필요한 라이브러리 불러오기\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, Embedding\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 스케일드 닷-프로덕트 어텐션\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights\n"
      ],
      "metadata": {
        "id": "x1K-FXzHCqBO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 멀티헤드 어텐션 레이어\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.wq = Dense(d_model)\n",
        "        self.wk = Dense(d_model)\n",
        "        self.wv = Dense(d_model)\n",
        "        self.dense = Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])  # [B, heads, seq_len, depth]\n",
        "\n",
        "    def call(self, q, k, v, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.split_heads(self.wq(q), batch_size)\n",
        "        k = self.split_heads(self.wk(k), batch_size)\n",
        "        v = self.split_heads(self.wv(v), batch_size)\n",
        "\n",
        "        scaled_attention, _ = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # [B, seq_len, heads, depth]\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.num_heads * self.depth))\n",
        "        output = self.dense(concat_attention)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "kwbTv3pAC4YH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 포지션 와이즈 피드포워드 네트워크\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        Dense(dff, activation='relu'),  # 확장\n",
        "        Dense(d_model)                  # 원래 차원으로 축소\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "RretRg6WDACb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 인코더 레이어\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        out1 = self.layernorm1(x + self.dropout1(attn_output, training=training))\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out2 = self.layernorm2(out1 + self.dropout2(ffn_output, training=training))\n",
        "        return out2\n"
      ],
      "metadata": {
        "id": "MJTOjuwBDHOu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 포지셔널 인코딩\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    def get_angles(pos, i, d_model):\n",
        "        angles = pos / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "        return angles\n",
        "\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                            np.arange(d_model)[np.newaxis, :],\n",
        "                            d_model)\n",
        "\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # even\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # odd\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "ufSo2AuQDMpD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 전체 인코더\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for enc_layer in self.enc_layers:\n",
        "            # Pass 'training' as a keyword argument\n",
        "            x = enc_layer(x, training=training, mask=mask)\n",
        "\n",
        "        return x  # [batch_size, input_seq_len, d_model]"
      ],
      "metadata": {
        "id": "gO1bYBQGDR9t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ======== (1) 인코더 클래스 및 관련 함수 정의 (위 코드 사용) ========\n",
        "# scaled_dot_product_attention, MultiHeadAttention, point_wise_feed_forward_network,\n",
        "# EncoderLayer, positional_encoding, Encoder\n",
        "# => 여기에 위에서 작성한 코드 그대로 복사해서 넣으세요.\n",
        "\n",
        "# ======== (2) 간단한 예제 설정 ========\n",
        "# 가상의 텍스트 입력 (숫자로 변환된 토큰들)\n",
        "sample_input = tf.constant([[1, 2, 3, 4, 0, 0]])  # [batch_size=1, seq_len=6]\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "num_layers = 2\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "dff = 256\n",
        "input_vocab_size = 1000\n",
        "max_pos_encoding = 100\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# ======== (3) 인코더 모델 생성 ========\n",
        "encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
        "                  input_vocab_size, max_pos_encoding, dropout_rate)\n",
        "\n",
        "# ======== (4) 출력 확인 ========\n",
        "output = encoder(sample_input, training=False)\n",
        "\n",
        "print(\"입력 시퀀스:\")\n",
        "print(sample_input.numpy())\n",
        "print(\"\\n인코더 출력 (shape: {}):\".format(output.shape))\n",
        "print(output.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IbuYryXDq3j",
        "outputId": "a31f7091-d9bd-48cd-8ea0-db91b16fc428"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 시퀀스:\n",
            "[[1 2 3 4 0 0]]\n",
            "\n",
            "인코더 출력 (shape: (1, 6, 64)):\n",
            "[[[-4.76292521e-03 -1.08566022e+00  2.85200998e-02  6.39593065e-01\n",
            "   -2.90596890e+00 -5.76012373e-01  7.56474257e-01  3.68133754e-01\n",
            "   -1.66109371e+00  1.72644258e+00  4.97078151e-03  1.18015218e+00\n",
            "    1.92148185e+00  3.89756441e-01  1.43419832e-01 -3.34532201e-01\n",
            "   -1.08149743e+00  1.21578038e-01 -2.36467868e-01  1.73903227e+00\n",
            "    1.34589624e+00 -5.03473461e-01 -1.82961822e+00  3.21822971e-01\n",
            "    9.41980243e-01  2.44196907e-01 -4.23899770e-01  1.56017140e-01\n",
            "   -3.99980664e-01 -6.05757982e-02 -4.19822156e-01  8.44729185e-01\n",
            "   -1.37926832e-01 -4.01581883e-01 -1.84646398e-01  2.05500650e+00\n",
            "    1.21487379e+00 -5.68237603e-01 -6.80037439e-01 -6.98643267e-01\n",
            "    6.54899031e-02  1.36365438e+00 -1.12029672e+00  2.11123109e+00\n",
            "   -4.97993052e-01 -6.70786798e-01 -7.34367013e-01  8.93040717e-01\n",
            "   -7.96531737e-01 -9.12059128e-01 -1.55349398e+00  1.58619002e-01\n",
            "   -6.14569783e-01  2.02495670e+00 -2.33574808e-01  5.68173170e-01\n",
            "   -1.50335360e+00 -1.01702243e-01 -4.48517442e-01  1.03768539e+00\n",
            "   -1.08106434e-01 -6.17975116e-01 -5.16479552e-01  2.57319272e-01]\n",
            "  [ 3.16824675e-01 -1.42599750e+00  2.54650891e-01  4.82218862e-01\n",
            "   -2.93605494e+00 -8.25924933e-01  9.63331282e-01  3.69936585e-01\n",
            "   -1.30311394e+00  1.83984387e+00  1.60199165e-01  1.08520925e+00\n",
            "    2.02495003e+00  3.59931409e-01  2.74354100e-01 -2.10551426e-01\n",
            "   -1.09674108e+00  8.94665122e-02 -4.85174954e-01  1.37324214e+00\n",
            "    1.25929403e+00 -7.86338568e-01 -1.61440682e+00  1.48993134e-01\n",
            "    1.11100340e+00  1.14639036e-01 -8.28539878e-02  3.77778560e-02\n",
            "   -3.94865572e-01 -1.79005429e-01 -3.31446230e-01  1.23875451e+00\n",
            "   -6.84869528e-01 -3.31617713e-01 -1.50189355e-01  2.17683554e+00\n",
            "    1.16421390e+00 -4.95031834e-01 -4.41266477e-01 -5.15613198e-01\n",
            "   -1.35946691e-01  1.54147160e+00 -1.30489564e+00  1.63050675e+00\n",
            "   -4.14879143e-01 -3.30626667e-01 -5.76014340e-01  8.19236398e-01\n",
            "   -1.09655201e+00 -8.13212633e-01 -1.33534014e+00  3.85781348e-01\n",
            "   -5.35420299e-01  1.78976095e+00 -3.08322370e-01  8.48286629e-01\n",
            "   -1.80803823e+00 -4.59579289e-01 -7.18031973e-02  7.76692688e-01\n",
            "   -2.95838714e-01 -8.66037726e-01 -6.46721840e-01  6.52886569e-01]\n",
            "  [ 6.10203683e-01 -1.90466321e+00  4.69237447e-01  3.47563550e-02\n",
            "   -2.60453916e+00 -1.00704551e+00  1.02677417e+00 -2.67425179e-03\n",
            "   -9.72279668e-01  1.95911646e+00  2.05390289e-01  7.21167624e-01\n",
            "    1.76501572e+00  3.01276714e-01  1.41543955e-01 -5.90093613e-01\n",
            "   -7.73805320e-01  1.49814919e-01 -7.17343152e-01  1.38402200e+00\n",
            "    1.39680171e+00 -7.20030546e-01 -1.78480244e+00  6.79977894e-01\n",
            "    1.13824737e+00 -2.37196311e-02 -1.21894084e-01 -6.82254508e-02\n",
            "   -1.98544115e-01 -2.44344845e-02 -8.48099813e-02  9.07974780e-01\n",
            "   -4.55164343e-01 -1.24953054e-01 -7.83091009e-01  2.32920766e+00\n",
            "    1.02893078e+00 -5.88978887e-01 -4.31844026e-01 -7.65712440e-01\n",
            "    9.23635066e-02  2.17411613e+00 -8.70405674e-01  1.34404182e+00\n",
            "   -2.54956186e-01 -7.57070184e-01 -6.50774658e-01  6.05157018e-01\n",
            "   -9.20411229e-01 -8.71908903e-01 -1.78902757e+00  6.59449995e-01\n",
            "   -9.65662837e-01  1.63677621e+00 -6.10630453e-01  8.36239636e-01\n",
            "   -1.24695230e+00 -4.38520700e-01 -1.02982320e-01  9.22961831e-01\n",
            "    1.50762796e-01 -7.04746723e-01 -3.13007712e-01  5.74377239e-01]\n",
            "  [ 1.85353190e-01 -2.07124257e+00  3.94113958e-01 -1.23302817e-01\n",
            "   -2.10108709e+00 -1.20337999e+00  1.15884733e+00 -1.45475730e-01\n",
            "   -1.06124616e+00  1.86192751e+00  3.54397714e-01  4.97761607e-01\n",
            "    1.88712180e+00  4.59830821e-01  2.30106980e-01 -7.60806650e-02\n",
            "   -7.54103482e-01  1.16076544e-02 -9.36750054e-01  1.42574513e+00\n",
            "    1.79083228e+00 -6.42652035e-01 -1.48099613e+00  6.02600098e-01\n",
            "    1.08066058e+00 -3.46595868e-02  2.26857029e-02  5.03811091e-02\n",
            "   -1.55367360e-01  1.21200547e-01 -3.74858558e-01  2.59860754e-01\n",
            "   -2.82572627e-01  7.31322318e-02 -9.95114684e-01  2.29322672e+00\n",
            "    1.07732618e+00 -3.57482791e-01 -5.98756909e-01 -5.90953946e-01\n",
            "   -9.68185365e-02  1.91583908e+00 -1.19005775e+00  1.31822884e+00\n",
            "   -6.40109181e-01 -6.76416576e-01 -8.11253071e-01  6.15403533e-01\n",
            "   -5.45977056e-01 -1.09672678e+00 -1.66125357e+00  8.51751447e-01\n",
            "   -6.53723657e-01  1.79171252e+00 -3.53999913e-01  5.47145605e-01\n",
            "   -1.45685089e+00 -6.10839128e-01 -3.28011513e-02  1.46179473e+00\n",
            "    3.28744173e-01 -1.03645480e+00 -3.88043404e-01  5.68069398e-01]\n",
            "  [-3.41359138e-01 -2.02575779e+00  1.83335811e-01 -5.64165950e-01\n",
            "   -2.22641611e+00 -1.70995247e+00  8.66169035e-01 -4.49714780e-01\n",
            "   -5.31102538e-01  1.26940751e+00  1.69412315e-01  5.02054930e-01\n",
            "    1.96831083e+00  7.68537045e-01  2.98367500e-01 -2.67455250e-01\n",
            "   -6.07806265e-01 -1.17501922e-01 -5.07239342e-01  1.62339211e+00\n",
            "    1.72552109e+00 -2.79941082e-01 -2.02228451e+00  3.97427499e-01\n",
            "    1.22142398e+00  1.67160451e-01  2.70140201e-01  2.03423768e-01\n",
            "    7.56359845e-02  2.46280789e-01 -4.32267785e-01  1.85555607e-01\n",
            "   -1.97932690e-01  1.49700224e-01 -7.88409233e-01  2.19168043e+00\n",
            "    1.55815971e+00 -1.37851089e-01 -4.38225627e-01 -5.40741384e-01\n",
            "    3.50493081e-02  2.12363124e+00 -8.24037969e-01  1.49126816e+00\n",
            "   -4.15251076e-01 -1.02634060e+00 -7.96898723e-01  7.87527084e-01\n",
            "   -8.96649301e-01 -5.90014398e-01 -1.73944533e+00  3.68619382e-01\n",
            "   -8.52791429e-01  1.81636047e+00 -5.24163783e-01  5.74878275e-01\n",
            "   -1.06451118e+00 -2.99792886e-01  1.15295500e-01  1.20381880e+00\n",
            "    1.00876018e-02 -1.03916132e+00 -4.88723278e-01  1.76273957e-01]\n",
            "  [-3.91401768e-01 -1.53832054e+00 -1.67622477e-01 -4.56840456e-01\n",
            "   -2.53354216e+00 -2.02945614e+00  8.12309921e-01 -5.32166123e-01\n",
            "   -5.14532268e-01  1.08686590e+00  1.28185153e-01  4.51074392e-01\n",
            "    2.04840660e+00  6.24686778e-01  3.93102378e-01 -3.91791612e-01\n",
            "   -5.29928923e-01 -1.75919771e-01 -3.46576959e-01  1.72457981e+00\n",
            "    1.65196013e+00 -3.13640296e-01 -2.01074767e+00  4.18753743e-01\n",
            "    1.25683832e+00  2.63848424e-01  3.95751208e-01  3.10798764e-01\n",
            "    1.03350416e-01  2.54334569e-01 -4.71376359e-01  4.48869690e-02\n",
            "   -3.88134569e-02  1.27829224e-01 -5.88078201e-01  2.15562344e+00\n",
            "    1.53339767e+00 -3.19209307e-01 -4.25154269e-01 -4.50780094e-01\n",
            "    1.94895089e-01  2.02182055e+00 -9.79176223e-01  1.55756521e+00\n",
            "   -3.95923376e-01 -1.06810975e+00 -8.52358162e-01  9.00117934e-01\n",
            "   -9.95551050e-01 -4.50487375e-01 -1.55093670e+00  2.06135631e-01\n",
            "   -9.34744775e-01  1.87803721e+00 -5.24113059e-01  6.29029810e-01\n",
            "   -1.09789252e+00 -1.60580799e-01  1.26467064e-01  1.13425255e+00\n",
            "   -1.19690880e-01 -8.94308984e-01 -4.65670049e-01  2.80537546e-01]]]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# 7. 전체 인코더\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, maximum_position_encoding, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for enc_layer in self.enc_layers:\n",
        "            # Pass 'training' as a keyword argument\n",
        "            x = enc_layer(x, training=training, mask=mask)\n",
        "\n",
        "        return x  # [batch_size, input_seq_len, d_model]"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "507M_ROfE43T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}